{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mshumer/gpt-prompt-engineer/blob/main/gpt_prompt_engineer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WljjH8K3s7kG"
   },
   "source": [
    "# Auto Prompt Engineering \n",
    "Original Notebook By Matt Shumer (https://twitter.com/mattshumer_)\n",
    "\n",
    "Original Github repo: https://github.com/mshumer/gpt-prompt-engineer\n",
    "\n",
    "In this notebook, we will simulate how to generate an optimal prompt/instruction for a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_src_code = '../'\n",
    "sys.path.append(path_to_src_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.functions import *\n",
    "from src.run_autoprompt import run_autoprompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Example Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load an example dataset from the datasets library that includes a question, ground truth, answer, and contexts. Make sure that ground truths is a list of contexts that are retrieved for the specific question. Press \"Yes\" when prompted to run the custom code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\")\n",
    "data = amnesty_qa['eval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section goes over how to call the Ragas library on the input dataset. This comes directly from the ragas library: https://github.com/explodinggradients/ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_correctness, context_precision, answer_similarity\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_aws import ChatBedrock\n",
    "from botocore.client import Config\n",
    "from langchain_community.embeddings import BedrockEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initiate Bedrock Chat and Bedrock Embeddings (the large language models used to generate evaluation metrics from Ragas). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"region_name\": \"us-east-1\",  # E.g. \"us-east-1\"\n",
    "    \"model_id\": 'anthropic.claude-3-haiku-20240307-v1:0',  # E.g \"anthropic.claude-v2\"\n",
    "    \"model_kwargs\": {'max_tokens':1000, 'temperature': 0.4,\n",
    "                                'stop_sequences': ['Question']},\n",
    "}\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 2})\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "bedrock_model = ChatBedrock(region_name=config['region_name'], model_id=config['model_id'],\n",
    "                            client=bedrock_client, model_kwargs=config['model_kwargs'])\n",
    "# init the embeddings\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    region_name=config[\"region_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will generate faithfulness, answer correctness, context precision, and answer similarity metrics. Some metrics will fail to parse output but that is ok, this is just to showcase an example of running Ragas on a sample dataset. Experiment with different versions of Bedrock to get more optimal outputs if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate(data,metrics=[faithfulness,answer_correctness, context_precision, answer_similarity], llm=bedrock_model,\n",
    "    embeddings=bedrock_embeddings);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.to_pandas().to_csv(\"../data/ragas_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJSSKFfV_X9F"
   },
   "source": [
    "## Automatic Prompt Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, automatic prompt generation is done using specific templates (available in instruction_generation_templates folder). Feel free to add more txt files to the list below based on the name of the templates in that folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"one-paragraph_instruction_with_demo.txt\", \n",
    "    \"one-sentence_instruction_with_demo.txt\", \n",
    "    \"step-by-step_instruction_with_demo.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command to obtain generated instructions. For a sample dataset of size 20 and 10 candidate instructions generated for each sample (in ../src/utils/config.py NUMBER_OF_PROMPTS=5), it takes about 3 minutes to run. Modify the NUMBER_OF_PROMPTS accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# use the csv file uploaded from Ragas as the input to autoprompt. \n",
    "run_autoprompts(True, files, path_to_ragas_outputs=\"../data/ragas_output.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output to running the autoprompt function above is stored in the data folder of this repository as 'new_prompts.csv'. In addition, a list of prompts are outputted in the data folder as 'prompt_id.csv'. Below is the output of the best three instructions for each question in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompts = pd.read_csv('../data/new_prompts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the prompts/instructions mapped to ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids = pd.read_csv('../data/prompt_ids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPMYK+Pn5QaRzPmh3T5a9ca",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
